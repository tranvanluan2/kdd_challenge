{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gym\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Value of Action ->  1\n",
      "Min Value of Action ->  0\n"
     ]
    }
   ],
   "source": [
    "num_states = 9\n",
    "num_actions = 4\n",
    "\n",
    "upper_bound = 1\n",
    "lower_bound = 0\n",
    "\n",
    "print(\"Max Value of Action ->  {}\".format(upper_bound))\n",
    "print(\"Min Value of Action ->  {}\".format(lower_bound))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OUActionNoise:\n",
    "    def __init__(self, mean, std_deviation, theta=0.15, dt=1e-2, x_initial=None):\n",
    "        self.theta = theta\n",
    "        self.mean = mean\n",
    "        self.std_dev = std_deviation\n",
    "        self.dt = dt\n",
    "        self.x_initial = x_initial\n",
    "        self.reset()\n",
    "\n",
    "    def __call__(self):\n",
    "        # Formula taken from https://www.wikipedia.org/wiki/Ornstein-Uhlenbeck_process.\n",
    "        x = (\n",
    "            self.x_prev\n",
    "            + self.theta * (self.mean - self.x_prev) * self.dt\n",
    "            + self.std_dev * np.sqrt(self.dt) * np.random.normal(size=self.mean.shape)\n",
    "        )\n",
    "        # Store x into x_prev\n",
    "        # Makes next noise dependent on current one\n",
    "        self.x_prev = x\n",
    "        return x\n",
    "\n",
    "    def reset(self):\n",
    "        if self.x_initial is not None:\n",
    "            self.x_prev = self.x_initial\n",
    "        else:\n",
    "            self.x_prev = np.zeros_like(self.mean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Buffer:\n",
    "    def __init__(self, buffer_capacity=100000, batch_size=64):\n",
    "\n",
    "        # Number of \"experiences\" to store at max\n",
    "        self.buffer_capacity = buffer_capacity\n",
    "        # Num of tuples to train on.\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # Its tells us num of times record() was called.\n",
    "        self.buffer_counter = 0\n",
    "\n",
    "        # Instead of list of tuples as the exp.replay concept go\n",
    "        # We use different np.arrays for each tuple element\n",
    "        self.state_buffer = np.zeros((self.buffer_capacity, num_states))\n",
    "        self.action_buffer = np.zeros((self.buffer_capacity, num_actions))\n",
    "        self.reward_buffer = np.zeros((self.buffer_capacity, 1))\n",
    "        self.next_state_buffer = np.zeros((self.buffer_capacity, num_states))\n",
    "\n",
    "    # Takes (s,a,r,s') obervation tuple as input\n",
    "    def record(self, obs_tuple):\n",
    "        # Set index to zero if buffer_capacity is exceeded,\n",
    "        # replacing old records\n",
    "        index = self.buffer_counter % self.buffer_capacity\n",
    "\n",
    "        self.state_buffer[index] = obs_tuple[0]\n",
    "        self.action_buffer[index] = obs_tuple[1]\n",
    "        self.reward_buffer[index] = obs_tuple[2]\n",
    "        self.next_state_buffer[index] = obs_tuple[3]\n",
    "\n",
    "        self.buffer_counter += 1\n",
    "\n",
    "    # We compute the loss and update parameters\n",
    "    def learn(self):\n",
    "        # Get sampling range\n",
    "        record_range = min(self.buffer_counter, self.buffer_capacity)\n",
    "        # Randomly sample indices\n",
    "        batch_indices = np.random.choice(record_range, self.batch_size)\n",
    "\n",
    "        # Convert to tensors\n",
    "        state_batch = tf.convert_to_tensor(self.state_buffer[batch_indices])\n",
    "        action_batch = tf.convert_to_tensor(self.action_buffer[batch_indices])\n",
    "        reward_batch = tf.convert_to_tensor(self.reward_buffer[batch_indices])\n",
    "        reward_batch = tf.cast(reward_batch, dtype=tf.float32)\n",
    "        next_state_batch = tf.convert_to_tensor(self.next_state_buffer[batch_indices])\n",
    "\n",
    "        # Training and updating Actor & Critic networks.\n",
    "        # See Pseudo Code.\n",
    "        with tf.GradientTape() as tape:\n",
    "            target_actions = target_actor(next_state_batch)\n",
    "            y = reward_batch + gamma * target_critic([next_state_batch, target_actions])\n",
    "            critic_value = critic_model([state_batch, action_batch])\n",
    "            critic_loss = tf.math.reduce_mean(tf.math.square(y - critic_value))\n",
    "\n",
    "        critic_grad = tape.gradient(critic_loss, critic_model.trainable_variables)\n",
    "        critic_optimizer.apply_gradients(\n",
    "            zip(critic_grad, critic_model.trainable_variables)\n",
    "        )\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            actions = actor_model(state_batch)\n",
    "            critic_value = critic_model([state_batch, actions])\n",
    "            # Used `-value` as we want to maximize the value given\n",
    "            # by the critic for our actions\n",
    "            actor_loss = -tf.math.reduce_mean(critic_value)\n",
    "\n",
    "        actor_grad = tape.gradient(actor_loss, actor_model.trainable_variables)\n",
    "        actor_optimizer.apply_gradients(\n",
    "            zip(actor_grad, actor_model.trainable_variables)\n",
    "        )\n",
    "\n",
    "\n",
    "# This update target parameters slowly\n",
    "# Based on rate `tau`, which is much less than one.\n",
    "def update_target(tau):\n",
    "    new_weights = []\n",
    "    target_variables = target_critic.weights\n",
    "    for i, variable in enumerate(critic_model.weights):\n",
    "        new_weights.append(variable * tau + target_variables[i] * (1 - tau))\n",
    "\n",
    "    target_critic.set_weights(new_weights)\n",
    "\n",
    "    new_weights = []\n",
    "    target_variables = target_actor.weights\n",
    "    for i, variable in enumerate(actor_model.weights):\n",
    "        new_weights.append(variable * tau + target_variables[i] * (1 - tau))\n",
    "\n",
    "    target_actor.set_weights(new_weights)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_actor():\n",
    "    # Initialize weights between -3e-3 and 3-e3\n",
    "    last_init = tf.random_uniform_initializer(minval=-0.003, maxval=0.003)\n",
    "\n",
    "    inputs = layers.Input(shape=(num_states,))\n",
    "    out = layers.Dense(6, activation=\"relu\")(inputs)\n",
    "    out = layers.BatchNormalization()(out)\n",
    "    out = layers.Dense(4, activation=\"relu\")(out)\n",
    "    out = layers.BatchNormalization()(out)\n",
    "    outputs = layers.Dense(4, activation=\"tanh\", kernel_initializer=last_init)(out)\n",
    "\n",
    "    # Our upper bound is 2.0 for Pendulum.\n",
    "    outputs = outputs * upper_bound\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_critic():\n",
    "    # State as input\n",
    "    state_input = layers.Input(shape=(num_states))\n",
    "    state_out = layers.Dense(6, activation=\"relu\")(state_input)\n",
    "    state_out = layers.BatchNormalization()(state_out)\n",
    "    state_out = layers.Dense(4, activation=\"relu\")(state_out)\n",
    "    state_out = layers.BatchNormalization()(state_out)\n",
    "\n",
    "    # Action as input\n",
    "    action_input = layers.Input(shape=(num_actions))\n",
    "    action_out = layers.Dense(4, activation=\"relu\")(action_input)\n",
    "    action_out = layers.BatchNormalization()(action_out)\n",
    "\n",
    "    # Both are passed through seperate layer before concatenating\n",
    "    concat = layers.Concatenate()([state_out, action_out])\n",
    "\n",
    "    out = layers.Dense(4, activation=\"relu\")(concat)\n",
    "    out = layers.BatchNormalization()(out)\n",
    "    out = layers.Dense(4, activation=\"relu\")(out)\n",
    "    out = layers.BatchNormalization()(out)\n",
    "    outputs = layers.Dense(1)(out)\n",
    "\n",
    "    # Outputs single value for give state-action\n",
    "    model = tf.keras.Model([state_input, action_input], outputs)\n",
    "\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def policy(state, noise_object):\n",
    "    sampled_actions = tf.squeeze(actor_model(state))\n",
    "    noise = noise_object()\n",
    "    # Adding noise to action\n",
    "    sampled_actions = sampled_actions.numpy() + noise\n",
    "\n",
    "    # We make sure action is within bounds\n",
    "    legal_action = np.clip(sampled_actions, lower_bound, upper_bound)\n",
    "\n",
    "    return [np.squeeze(legal_action)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_dev = 0.1\n",
    "ou_noise = OUActionNoise(mean=np.zeros(1), std_deviation=float(std_dev) * np.ones(1))\n",
    "\n",
    "actor_model = get_actor()\n",
    "critic_model = get_critic()\n",
    "\n",
    "target_actor = get_actor()\n",
    "target_critic = get_critic()\n",
    "\n",
    "# Making the weights equal initially\n",
    "target_actor.set_weights(actor_model.get_weights())\n",
    "target_critic.set_weights(critic_model.get_weights())\n",
    "\n",
    "# Learning rate for actor-critic models\n",
    "critic_lr = 0.0005\n",
    "actor_lr = 0.0003\n",
    "\n",
    "critic_optimizer = tf.keras.optimizers.Adam(critic_lr)\n",
    "actor_optimizer = tf.keras.optimizers.Adam(actor_lr)\n",
    "\n",
    "total_episodes = 1000\n",
    "# Discount factor for future rewards\n",
    "gamma = 0.99\n",
    "# Used to update target networks\n",
    "tau = 0.005\n",
    "\n",
    "buffer = Buffer(50000, 32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper functions using simulator\n",
    "import simulator\n",
    "import os\n",
    "import json\n",
    "import math\n",
    "import numpy as np\n",
    "import random\n",
    "from shutil import copyfile\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pickle\n",
    "\n",
    "lambda_h = 1.0\n",
    "lambda_i = 0.5\n",
    "lambda_q = 0.3\n",
    "lambda_c = 0.2\n",
    "\n",
    "def compute_score(I, Q):\n",
    "\ttheta_i = 1000.0\n",
    "\ttheta_q = 100000.0\n",
    "\tq_w = 1.0\n",
    "\n",
    "\treturn math.exp(I*1.0/theta_i) + q_w * math.exp(Q*1.0/theta_q)\n",
    "\n",
    "def prob_infected_from_acq(user_id, pc=0.0015):\n",
    "\n",
    "\tresidental_acq = engine.get_individual_residential_acq(user_id)\n",
    "\tworking_acq = engine.get_individual_working_acq(user_id)\n",
    "\n",
    "\tarea_ids = engine.get_individual_visited_history(user_id)\n",
    "\tcount_same_places = [0 for _ in area_ids]\n",
    "\tfor acq_id in residental_acq:\n",
    "\t\tacq_area_ids = engine.get_individual_visited_history(acq_id)\n",
    "\t\tfor idx, (area_id, acq_area_id) in enumerate(zip(area_ids, acq_area_ids)):\n",
    "\t\t\tif area_id == acq_area_id and area_id > -1:\n",
    "\t\t\t\tcount_same_places[idx] +=1\n",
    "\n",
    "\tfor acq_id in working_acq:\n",
    "\t\tacq_area_ids = engine.get_individual_visited_history(acq_id)\n",
    "\t\tfor idx, (area_id, acq_area_id) in enumerate(zip(area_ids, acq_area_ids)):\n",
    "\t\t\tif area_id == acq_area_id and area_id > -1: \n",
    "\t\t\t\tcount_same_places[idx] +=1\n",
    "\n",
    "\tp = 1 #prob of not being transmitted\n",
    "\tfor c in count_same_places:\n",
    "\t\tif c > 0:\n",
    "\t\t\tp = p*((1-pc)**c)\n",
    "\treturn 1-p\n",
    "\n",
    "def interven(user_id, engine, isolation_threshold, quarantine_threshold, confine_threshold, confine_f2_threshold,\n",
    "\tisolation_day=1, quarantine_day=1, confine_day=1, confine_f2_day=1):\n",
    "\tinfection_state = engine.get_individual_infection_state(user_id)\n",
    "\tif infection_state  == 3 or infection_state == 4:\n",
    "\n",
    "\t\tresidental_acq = engine.get_individual_residential_acq(user_id)\n",
    "\t\tworking_acq = engine.get_individual_working_acq(user_id)\n",
    "\n",
    "\t\t#act the acq\n",
    "\t\tfor acq_id in residental_acq:\n",
    "\t\t\tif engine.get_individual_intervention_state(acq_id) == 1:\n",
    "\t\t\t\tprob_infected = prob_infected_from_acq(acq_id)\n",
    "\t\t\t\t# print(\"Prob of being infected = \", prob_infected)\n",
    "\t\t\t\t# print(\"*******************************************+++++++++++++++++++++\")\n",
    "\t\t\t\tif prob_infected > isolation_threshold:\n",
    "\t\t\t\t\tengine.set_individual_isolate_days({acq_id: isolation_day})\n",
    "\t\t\t\t\t\n",
    "\t\t\t\telif prob_infected > quarantine_threshold:\n",
    "\t\t\t\t\tengine.set_individual_quarantine_days({acq_id: quarantine_day})\n",
    "\t\t\t\t\t\n",
    "\t\t\t\telif prob_infected > confine_threshold:\n",
    "\t\t\t\t\tengine.set_individual_confine_days({acq_id: confine_day})\n",
    "\t\t\t\t\t\n",
    "\t\t\t\t#quarantine f2\n",
    "\t\t\t\tif prob_infected > confine_f2_threshold:\n",
    "\t\t\t\t\tresidental_acq_f2 = engine.get_individual_residential_acq(acq_id)\n",
    "\t\t\t\t\tworking_acq_f2 = engine.get_individual_working_acq(acq_id)\n",
    "\n",
    "\t\t\t\t\t\n",
    "\t\t\t\t\tfor f2 in residental_acq_f2:\n",
    "\t\t\t\t\t\tif engine.get_individual_intervention_state(f2) == 1:\n",
    "\t\t\t\t\t\t\t# engine.set_individual_quarantine_days({f2: quarantine_day})\n",
    "\t\t\t\t\t\t\tengine.set_individual_confine_days({f2: confine_f2_day})\n",
    "\t\t\t\t\t\t\t\n",
    "\t\t\t\t\tfor f2 in working_acq_f2:\n",
    "\t\t\t\t\t\tif engine.get_individual_intervention_state(f2) == 1:\n",
    "\t\t\t\t\t\t\t# engine.set_individual_quarantine_days({f2: quarantine_day})\n",
    "\t\t\t\t\t\t\tengine.set_individual_confine_days({f2: confine_f2_day})\n",
    "\t\t\t\t\t\t\t\t\n",
    "\n",
    "\t\tfor acq_id in working_acq:\n",
    "\t\t\tif engine.get_individual_intervention_state(acq_id) == 1:\n",
    "\t\t\t\tprob_infected = prob_infected_from_acq(acq_id)\n",
    "\t\t\t\t# print(\"Prob of being infected = \", prob_infected)\n",
    "\t\t\t\t# print(\"*******************************************+++++++++++++++++++++\")\n",
    "\t\t\t\tif prob_infected > isolation_threshold:\n",
    "\t\t\t\t\tengine.set_individual_isolate_days({acq_id: isolation_day})\n",
    "\t\t\t\t\t\n",
    "\t\t\t\telif prob_infected > quarantine_threshold:\n",
    "\t\t\t\t\tengine.set_individual_quarantine_days({acq_id: quarantine_day})\n",
    "\t\t\t\t\t\n",
    "\t\t\t\telif prob_infected > confine_threshold:\n",
    "\t\t\t\t\tengine.set_individual_confine_days({acq_id: confine_day})\n",
    "\t\t\t\t\t\n",
    "\t\t\t\tif prob_infected > confine_f2_threshold:\n",
    "\t\t\t\t\t#quarantine f2\n",
    "\t\t\t\t\tresidental_acq_f2 = engine.get_individual_residential_acq(acq_id)\n",
    "\t\t\t\t\tworking_acq_f2 = engine.get_individual_working_acq(acq_id)\n",
    "\n",
    "\t\t\t\t\t\n",
    "\t\t\t\t\tfor f2 in residental_acq_f2:\n",
    "\t\t\t\t\t\tif engine.get_individual_intervention_state(f2) == 1:\n",
    "\t\t\t\t\t\t\t# engine.set_individual_quarantine_days({f2: quarantine_day})\n",
    "\t\t\t\t\t\t\tengine.set_individual_confine_days({f2: confine_f2_day})\n",
    "\t\t\t\t\t\t\t\n",
    "\t\t\t\t\tfor f2 in working_acq_f2:\n",
    "\t\t\t\t\t\tif engine.get_individual_intervention_state(f2) == 1:\n",
    "\t\t\t\t\t\t\t# engine.set_individual_quarantine_days({f2: quarantine_day})\n",
    "\t\t\t\t\t\t\tengine.set_individual_confine_days({f2: confine_f2_day})\n",
    "\t\t\t\t\t\t\t\t\n",
    "\n",
    "\n",
    "\tif (infection_state == 3 or infection_state == 4):\n",
    "\t\tif engine.get_individual_intervention_state(user_id) < 5:\n",
    "\t\t\t#hospitalize user\n",
    "\t\t\t\n",
    "\t\t\tengine.set_individual_to_treat({user_id: True})\n",
    "\n",
    "            \n",
    "def get_state(engine):\n",
    "    cur_day = engine.get_current_day()*1.0/60.0 #normalize\n",
    "    total_case = 0\n",
    "    new_case = 0\n",
    "    f1 = set()\n",
    "    f2 = set()\n",
    "    for user_id in range(10000):\n",
    "        infection_state = engine.get_individual_infection_state(user_id)\n",
    "        if infection_state  == 3 or infection_state == 4:\n",
    "            total_case += 1\n",
    "            if engine.get_individual_intervention_state(user_id) < 5:\n",
    "                new_case +=1\n",
    "                \n",
    "            residental_acq = engine.get_individual_residential_acq(user_id)\n",
    "            working_acq = engine.get_individual_working_acq(user_id)\n",
    "            f1.update(residental_acq)\n",
    "            f1.update(working_acq)\n",
    "            \n",
    "            for acq in residental_acq:\n",
    "                f2.update([x for x in engine.get_individual_residential_acq(acq) if x not in f1])\n",
    "                \n",
    "                f2.update([x for x in engine.get_individual_working_acq(acq) if x not in f1])\n",
    "            for acq in working_acq:\n",
    "                f2.update([x for x in engine.get_individual_residential_acq(acq) if x not in f1])\n",
    "                f2.update([x for x in engine.get_individual_working_acq(acq) if x not in f1])\n",
    "    all_prob_f1 = [prob_infected_from_acq(x) for x in f1]\n",
    "    if len(all_prob_f1) == 0:\n",
    "        all_prob_f1 = [0]\n",
    "            \n",
    "    return [cur_day, total_case*1.0/10000, new_case*1.0/10000, len(f1)*1.0/10000, len(f2)*1.0/10000, \n",
    "           np.mean(all_prob_f1), np.min(all_prob_f1), np.max(all_prob_f1), np.median(all_prob_f1)]\n",
    "\n",
    "            \n",
    "                \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode * 0 * Avg Reward is ==> -0.3670430505541322\n",
      "Episode * 1 * Avg Reward is ==> -0.34874998908356813\n",
      "Episode * 2 * Avg Reward is ==> -0.37553546641960295\n",
      "Episode * 3 * Avg Reward is ==> -0.4149709573114735\n",
      "Episode * 4 * Avg Reward is ==> -0.45507946448239006\n",
      "Episode * 5 * Avg Reward is ==> -0.44498652410611056\n",
      "Episode * 6 * Avg Reward is ==> -0.4707889190561095\n",
      "Episode * 7 * Avg Reward is ==> -0.47297828114151025\n",
      "Episode * 8 * Avg Reward is ==> -0.4730149771399233\n",
      "Episode * 9 * Avg Reward is ==> -0.46982660990810887\n",
      "Episode * 10 * Avg Reward is ==> -0.4818130181027964\n",
      "Episode * 11 * Avg Reward is ==> -0.48536132195742643\n"
     ]
    }
   ],
   "source": [
    "# To store reward history of each episode\n",
    "ep_reward_list = []\n",
    "# To store average reward history of last few episodes\n",
    "avg_reward_list = []\n",
    "\n",
    "# Takes about ? min to train\n",
    "for ep in range(total_episodes):\n",
    "    \n",
    "    #create simulator\n",
    "    I , Q = 0,0 #for computing score\n",
    "    engine = simulator.Engine(thread_num=1, write_mode=\"append\", specified_run_name=\"test\")\n",
    "    engine.reset()\n",
    "    score = compute_score(I, Q)\n",
    "    \n",
    "    #compute state \n",
    "    prev_state = get_state(engine)\n",
    "    episodic_reward = 0\n",
    "    period = 840\n",
    "    infected_set = set()\n",
    "    for i in range(period):\n",
    "        # Uncomment this to see the Actor in action\n",
    "        # But not in a python notebook.\n",
    "        # env.render()\n",
    "        if i % 14 == 0:\n",
    "            tf_prev_state = tf.expand_dims(tf.convert_to_tensor(prev_state), 0)\n",
    "\n",
    "            action = policy(tf_prev_state, ou_noise)[0]\n",
    "#             print(\"action = \", action)\n",
    "            \n",
    "            #apply action to environment\n",
    "            [isolation_threshold, quarantine_threshold, confine_threshold, confine_f2_threshold] = action\n",
    "#             print(\"isolation threshold = \", isolation_threshold)\n",
    "            for user_id in range(10000):\n",
    "                infection_state = engine.get_individual_infection_state(user_id)\n",
    "                if infection_state  == 3 or infection_state == 4:\n",
    "                    infected_set.add(user_id)\n",
    "                interven(user_id,engine, isolation_threshold, quarantine_threshold, \n",
    "                         confine_threshold, confine_f2_threshold)\n",
    "            I = len(infected_set)\n",
    "            Q += lambda_h * engine.get_hospitalize_count()\n",
    "            Q += lambda_i * engine.get_isolate_count()\n",
    "            Q += lambda_q * engine.get_quarantine_count()\n",
    "            Q += lambda_c * engine.get_confine_count()\n",
    "            new_score = compute_score(I, Q)\n",
    "            reward = score - new_score\n",
    "            score = new_score\n",
    "            state = get_state(engine)\n",
    "        \n",
    "            # Recieve state and reward from environment.\n",
    "#             state, reward, done, info = env.step(action)\n",
    "\n",
    "            buffer.record((prev_state, action, reward, state))\n",
    "            episodic_reward += reward\n",
    "\n",
    "            buffer.learn()\n",
    "            update_target(tau)\n",
    "\n",
    "\n",
    "            prev_state = state\n",
    "        #next time step\n",
    "        engine.next_step()\n",
    "        engine.get_current_time()\n",
    "\n",
    "    ep_reward_list.append(episodic_reward)\n",
    "\n",
    "    # Mean of last 40 episodes\n",
    "    avg_reward = np.mean(ep_reward_list[-40:])\n",
    "    print(\"Episode * {} * Avg Reward is ==> {}\".format(ep, avg_reward))\n",
    "    avg_reward_list.append(avg_reward)\n",
    "\n",
    "# Plotting graph\n",
    "# Episodes versus Avg. Rewards\n",
    "plt.plot(avg_reward_list)\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Avg. Epsiodic Reward\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the weights\n",
    "actor_model.save_weights(\"actor.h5\")\n",
    "critic_model.save_weights(\"critic.h5\")\n",
    "\n",
    "target_actor.save_weights(\"target_actor.h5\")\n",
    "target_critic.save_weights(\"target_critic.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
